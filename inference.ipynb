{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Submission"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "First, you must install the following libraries in your environment"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Requirement already satisfied: pandas in /home/s_thomine/anaconda3/envs/hemat/lib/python3.12/site-packages (2.2.3)\n",
      "Requirement already satisfied: numpy>=1.26.0 in /home/s_thomine/anaconda3/envs/hemat/lib/python3.12/site-packages (from pandas) (2.1.2)\n",
      "Requirement already satisfied: python-dateutil>=2.8.2 in /home/s_thomine/anaconda3/envs/hemat/lib/python3.12/site-packages (from pandas) (2.9.0.post0)\n",
      "Requirement already satisfied: pytz>=2020.1 in /home/s_thomine/anaconda3/envs/hemat/lib/python3.12/site-packages (from pandas) (2024.2)\n",
      "Requirement already satisfied: tzdata>=2022.7 in /home/s_thomine/anaconda3/envs/hemat/lib/python3.12/site-packages (from pandas) (2024.2)\n",
      "Requirement already satisfied: six>=1.5 in /home/s_thomine/anaconda3/envs/hemat/lib/python3.12/site-packages (from python-dateutil>=2.8.2->pandas) (1.17.0)\n",
      "Requirement already satisfied: tqdm in /home/s_thomine/anaconda3/envs/hemat/lib/python3.12/site-packages (4.66.5)\n",
      "Requirement already satisfied: pillow in /home/s_thomine/anaconda3/envs/hemat/lib/python3.12/site-packages (11.0.0)\n",
      "Requirement already satisfied: ultralytics in /home/s_thomine/anaconda3/envs/hemat/lib/python3.12/site-packages (8.3.38)\n",
      "Requirement already satisfied: numpy>=1.23.0 in /home/s_thomine/anaconda3/envs/hemat/lib/python3.12/site-packages (from ultralytics) (2.1.2)\n",
      "Requirement already satisfied: matplotlib>=3.3.0 in /home/s_thomine/anaconda3/envs/hemat/lib/python3.12/site-packages (from ultralytics) (3.9.2)\n",
      "Requirement already satisfied: opencv-python>=4.6.0 in /home/s_thomine/anaconda3/envs/hemat/lib/python3.12/site-packages (from ultralytics) (4.10.0.84)\n",
      "Requirement already satisfied: pillow>=7.1.2 in /home/s_thomine/anaconda3/envs/hemat/lib/python3.12/site-packages (from ultralytics) (11.0.0)\n",
      "Requirement already satisfied: pyyaml>=5.3.1 in /home/s_thomine/anaconda3/envs/hemat/lib/python3.12/site-packages (from ultralytics) (6.0.2)\n",
      "Requirement already satisfied: requests>=2.23.0 in /home/s_thomine/anaconda3/envs/hemat/lib/python3.12/site-packages (from ultralytics) (2.32.3)\n",
      "Requirement already satisfied: scipy>=1.4.1 in /home/s_thomine/anaconda3/envs/hemat/lib/python3.12/site-packages (from ultralytics) (1.14.1)\n",
      "Requirement already satisfied: torch>=1.8.0 in /home/s_thomine/anaconda3/envs/hemat/lib/python3.12/site-packages (from ultralytics) (2.5.0)\n",
      "Requirement already satisfied: torchvision>=0.9.0 in /home/s_thomine/anaconda3/envs/hemat/lib/python3.12/site-packages (from ultralytics) (0.20.0)\n",
      "Requirement already satisfied: tqdm>=4.64.0 in /home/s_thomine/anaconda3/envs/hemat/lib/python3.12/site-packages (from ultralytics) (4.66.5)\n",
      "Requirement already satisfied: psutil in /home/s_thomine/anaconda3/envs/hemat/lib/python3.12/site-packages (from ultralytics) (6.1.0)\n",
      "Requirement already satisfied: py-cpuinfo in /home/s_thomine/anaconda3/envs/hemat/lib/python3.12/site-packages (from ultralytics) (9.0.0)\n",
      "Requirement already satisfied: pandas>=1.1.4 in /home/s_thomine/anaconda3/envs/hemat/lib/python3.12/site-packages (from ultralytics) (2.2.3)\n",
      "Requirement already satisfied: seaborn>=0.11.0 in /home/s_thomine/anaconda3/envs/hemat/lib/python3.12/site-packages (from ultralytics) (0.13.2)\n",
      "Requirement already satisfied: ultralytics-thop>=2.0.0 in /home/s_thomine/anaconda3/envs/hemat/lib/python3.12/site-packages (from ultralytics) (2.0.13)\n",
      "Requirement already satisfied: contourpy>=1.0.1 in /home/s_thomine/anaconda3/envs/hemat/lib/python3.12/site-packages (from matplotlib>=3.3.0->ultralytics) (1.3.1)\n",
      "Requirement already satisfied: cycler>=0.10 in /home/s_thomine/anaconda3/envs/hemat/lib/python3.12/site-packages (from matplotlib>=3.3.0->ultralytics) (0.12.1)\n",
      "Requirement already satisfied: fonttools>=4.22.0 in /home/s_thomine/anaconda3/envs/hemat/lib/python3.12/site-packages (from matplotlib>=3.3.0->ultralytics) (4.55.3)\n",
      "Requirement already satisfied: kiwisolver>=1.3.1 in /home/s_thomine/anaconda3/envs/hemat/lib/python3.12/site-packages (from matplotlib>=3.3.0->ultralytics) (1.4.7)\n",
      "Requirement already satisfied: packaging>=20.0 in /home/s_thomine/anaconda3/envs/hemat/lib/python3.12/site-packages (from matplotlib>=3.3.0->ultralytics) (24.2)\n",
      "Requirement already satisfied: pyparsing>=2.3.1 in /home/s_thomine/anaconda3/envs/hemat/lib/python3.12/site-packages (from matplotlib>=3.3.0->ultralytics) (3.2.0)\n",
      "Requirement already satisfied: python-dateutil>=2.7 in /home/s_thomine/anaconda3/envs/hemat/lib/python3.12/site-packages (from matplotlib>=3.3.0->ultralytics) (2.9.0.post0)\n",
      "Requirement already satisfied: pytz>=2020.1 in /home/s_thomine/anaconda3/envs/hemat/lib/python3.12/site-packages (from pandas>=1.1.4->ultralytics) (2024.2)\n",
      "Requirement already satisfied: tzdata>=2022.7 in /home/s_thomine/anaconda3/envs/hemat/lib/python3.12/site-packages (from pandas>=1.1.4->ultralytics) (2024.2)\n",
      "Requirement already satisfied: charset-normalizer<4,>=2 in /home/s_thomine/anaconda3/envs/hemat/lib/python3.12/site-packages (from requests>=2.23.0->ultralytics) (3.4.0)\n",
      "Requirement already satisfied: idna<4,>=2.5 in /home/s_thomine/anaconda3/envs/hemat/lib/python3.12/site-packages (from requests>=2.23.0->ultralytics) (3.10)\n",
      "Requirement already satisfied: urllib3<3,>=1.21.1 in /home/s_thomine/anaconda3/envs/hemat/lib/python3.12/site-packages (from requests>=2.23.0->ultralytics) (2.2.3)\n",
      "Requirement already satisfied: certifi>=2017.4.17 in /home/s_thomine/anaconda3/envs/hemat/lib/python3.12/site-packages (from requests>=2.23.0->ultralytics) (2024.8.30)\n",
      "Requirement already satisfied: filelock in /home/s_thomine/anaconda3/envs/hemat/lib/python3.12/site-packages (from torch>=1.8.0->ultralytics) (3.16.1)\n",
      "Requirement already satisfied: typing-extensions>=4.8.0 in /home/s_thomine/anaconda3/envs/hemat/lib/python3.12/site-packages (from torch>=1.8.0->ultralytics) (4.12.2)\n",
      "Requirement already satisfied: networkx in /home/s_thomine/anaconda3/envs/hemat/lib/python3.12/site-packages (from torch>=1.8.0->ultralytics) (3.4.2)\n",
      "Requirement already satisfied: jinja2 in /home/s_thomine/anaconda3/envs/hemat/lib/python3.12/site-packages (from torch>=1.8.0->ultralytics) (3.1.4)\n",
      "Requirement already satisfied: fsspec in /home/s_thomine/anaconda3/envs/hemat/lib/python3.12/site-packages (from torch>=1.8.0->ultralytics) (2024.9.0)\n",
      "Requirement already satisfied: nvidia-cuda-nvrtc-cu12==12.4.127 in /home/s_thomine/anaconda3/envs/hemat/lib/python3.12/site-packages (from torch>=1.8.0->ultralytics) (12.4.127)\n",
      "Requirement already satisfied: nvidia-cuda-runtime-cu12==12.4.127 in /home/s_thomine/anaconda3/envs/hemat/lib/python3.12/site-packages (from torch>=1.8.0->ultralytics) (12.4.127)\n",
      "Requirement already satisfied: nvidia-cuda-cupti-cu12==12.4.127 in /home/s_thomine/anaconda3/envs/hemat/lib/python3.12/site-packages (from torch>=1.8.0->ultralytics) (12.4.127)\n",
      "Requirement already satisfied: nvidia-cudnn-cu12==9.1.0.70 in /home/s_thomine/anaconda3/envs/hemat/lib/python3.12/site-packages (from torch>=1.8.0->ultralytics) (9.1.0.70)\n",
      "Requirement already satisfied: nvidia-cublas-cu12==12.4.5.8 in /home/s_thomine/anaconda3/envs/hemat/lib/python3.12/site-packages (from torch>=1.8.0->ultralytics) (12.4.5.8)\n",
      "Requirement already satisfied: nvidia-cufft-cu12==11.2.1.3 in /home/s_thomine/anaconda3/envs/hemat/lib/python3.12/site-packages (from torch>=1.8.0->ultralytics) (11.2.1.3)\n",
      "Requirement already satisfied: nvidia-curand-cu12==10.3.5.147 in /home/s_thomine/anaconda3/envs/hemat/lib/python3.12/site-packages (from torch>=1.8.0->ultralytics) (10.3.5.147)\n",
      "Requirement already satisfied: nvidia-cusolver-cu12==11.6.1.9 in /home/s_thomine/anaconda3/envs/hemat/lib/python3.12/site-packages (from torch>=1.8.0->ultralytics) (11.6.1.9)\n",
      "Requirement already satisfied: nvidia-cusparse-cu12==12.3.1.170 in /home/s_thomine/anaconda3/envs/hemat/lib/python3.12/site-packages (from torch>=1.8.0->ultralytics) (12.3.1.170)\n",
      "Requirement already satisfied: nvidia-nccl-cu12==2.21.5 in /home/s_thomine/anaconda3/envs/hemat/lib/python3.12/site-packages (from torch>=1.8.0->ultralytics) (2.21.5)\n",
      "Requirement already satisfied: nvidia-nvtx-cu12==12.4.127 in /home/s_thomine/anaconda3/envs/hemat/lib/python3.12/site-packages (from torch>=1.8.0->ultralytics) (12.4.127)\n",
      "Requirement already satisfied: nvidia-nvjitlink-cu12==12.4.127 in /home/s_thomine/anaconda3/envs/hemat/lib/python3.12/site-packages (from torch>=1.8.0->ultralytics) (12.4.127)\n",
      "Requirement already satisfied: triton==3.1.0 in /home/s_thomine/anaconda3/envs/hemat/lib/python3.12/site-packages (from torch>=1.8.0->ultralytics) (3.1.0)\n",
      "Requirement already satisfied: setuptools in /home/s_thomine/anaconda3/envs/hemat/lib/python3.12/site-packages (from torch>=1.8.0->ultralytics) (75.1.0)\n",
      "Requirement already satisfied: sympy==1.13.1 in /home/s_thomine/anaconda3/envs/hemat/lib/python3.12/site-packages (from torch>=1.8.0->ultralytics) (1.13.1)\n",
      "Requirement already satisfied: mpmath<1.4,>=1.1.0 in /home/s_thomine/anaconda3/envs/hemat/lib/python3.12/site-packages (from sympy==1.13.1->torch>=1.8.0->ultralytics) (1.3.0)\n",
      "Requirement already satisfied: six>=1.5 in /home/s_thomine/anaconda3/envs/hemat/lib/python3.12/site-packages (from python-dateutil>=2.7->matplotlib>=3.3.0->ultralytics) (1.17.0)\n",
      "Requirement already satisfied: MarkupSafe>=2.0 in /home/s_thomine/anaconda3/envs/hemat/lib/python3.12/site-packages (from jinja2->torch>=1.8.0->ultralytics) (3.0.2)\n"
     ]
    }
   ],
   "source": [
    "# Needed installations\n",
    "!pip3 install pandas\n",
    "!pip3 install tqdm\n",
    "!pip3 install pillow\n",
    "!pip3 install ultralytics # install automatically the last version of PyTorch and Torchvision"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import shutil\n",
    "import time\n",
    "from collections import defaultdict\n",
    "\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "from PIL import Image\n",
    "from tqdm import tqdm\n",
    "\n",
    "import torch\n",
    "import torch.nn.functional as F\n",
    "import torchvision\n",
    "import torchvision.transforms as T\n",
    "from torchvision.ops import nms\n",
    "\n",
    "from ultralytics import YOLO\n",
    "from ultralytics.data.augment import LetterBox\n",
    "from ultralytics.utils.ops import xywh2xyxy\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Inference class for our yolo model"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Yolo model"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "For this submission, we opted for the latest YOLO model from Ultralytics: [YOLO11](https://docs.ultralytics.com/models/yolo11/)  alongside its predecessor [YOLOv10](https://docs.ultralytics.com/models/yolov10/). This decision was driven by the models' fast inference capabilities, which meet the challenge's requirement for efficient image processing. By leveraging different models, we aim to take advantage of ensembling diverse architectures to improve overall performance.\n",
    "\n",
    "We specifically chose relatively small versions of YOLO11 and YOLOv10 to maintain a balance between inference speed and accuracy: YOLO11n (size 384), YOLO11m (size 384), and YOLOv10n (size 384).\n",
    "\n",
    "All models were fine-tuned using an automated dataset curation process. Details of this process are provided in the training section of the notebook, where we outline our strategy for optimizing YOLO models to achieve the best possible performance."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Non-Maximum-Suppression"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The YOLO model from Ultralytics includes its own Non-Maximum Suppression (NMS) algorithm, designed to eliminate overlapping bounding boxes of the same class. We have chosen a relatively low NMS threshold of 0.4, as cells of the same or different classes should not overlap (at least, they do not in the training data).\n",
    "\n",
    "**Why?** In the context of cytology, one of the goals is to minimize the overlap between cells. After analyzing the dataset, we concluded that overlapping cells are either non-existent or extremely rare.   \n",
    "\n",
    "\n",
    "<img src=\"images/bbox.png\" alt=\"Bounding boxes overlap\" height=\"350\"/>  <img src=\"images/iou.png\" alt=\"IoU train data\" height=\"350\"/>  "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The standard Ultralytics functions do not return class probabilities for each detection, which is crucial for ensembling. To address this, we modified the inference process. First, we used the underlying Torch model to obtain the predictions in the correct format. Then, we modified the non_max_suppression function from Ultralytics to return the probability vector along with the usual outputs.\n",
    "\n",
    "Additionally, we handled the image preprocessing ourselves.\n",
    "\n",
    "Since YOLOv10 is end-to-end, we did not use the class probabilities when ensembling with YOLOv10 models."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Modified version of the non_max_suppression function from the ultralytics library\n",
    "def non_max_suppression_modified(\n",
    "    prediction,\n",
    "    conf_thres=0.25,\n",
    "    iou_thres=0.45,\n",
    "    classes=None,\n",
    "    agnostic=False,\n",
    "    max_det=300,\n",
    "    nc=0,  # number of classes (optional)\n",
    "    max_nms=30000,\n",
    "    max_wh=7680,\n",
    "    in_place=True,\n",
    "    rotated=False,\n",
    "):\n",
    "    # Checks\n",
    "    assert 0 <= conf_thres <= 1, f\"Invalid Confidence threshold {conf_thres}, valid values are between 0.0 and 1.0\"\n",
    "    assert 0 <= iou_thres <= 1, f\"Invalid IoU {iou_thres}, valid values are between 0.0 and 1.0\"\n",
    "    if isinstance(prediction, (list, tuple)):  # YOLOv8 model in validation model, output = (inference_out, loss_out)\n",
    "        prediction = prediction[0]  # select only inference output\n",
    "    if classes is not None:\n",
    "        classes = torch.tensor(classes, device=prediction.device)\n",
    "\n",
    "    bs = prediction.shape[0]  # batch size (BCN, i.e. 1,84,6300)\n",
    "    nc = nc or (prediction.shape[1] - 4)  # number of classes\n",
    "    nm = prediction.shape[1] - nc - 4  # number of masks\n",
    "    mi = 4 + nc  # mask start index\n",
    "    xc = prediction[:, 4:mi].amax(1) > conf_thres  # candidates\n",
    "\n",
    "    # Settings\n",
    "    # min_wh = 2  # (pixels) minimum box width and height\n",
    "\n",
    "    prediction = prediction.transpose(-1, -2)  # shape(1,84,6300) to shape(1,6300,84)\n",
    "    if not rotated:\n",
    "        if in_place:\n",
    "            prediction[..., :4] = xywh2xyxy(prediction[..., :4])  # xywh to xyxy\n",
    "        else:\n",
    "            prediction = torch.cat((xywh2xyxy(prediction[..., :4]), prediction[..., 4:]), dim=-1)  # xywh to xyxy\n",
    "\n",
    "    t = time.time()\n",
    "\n",
    "    output = [torch.zeros((0, 6 + nm), device=prediction.device)] * bs\n",
    "    output_oh= [torch.zeros((0, 4 + nc), device=prediction.device)] * bs\n",
    "\n",
    "    for xi, x in enumerate(prediction):  # image index, image inference\n",
    "        # Apply constraints\n",
    "        # x[((x[:, 2:4] < min_wh) | (x[:, 2:4] > max_wh)).any(1), 4] = 0  # width-height\n",
    "        \n",
    "        # get the elements of the image that have a confidence above the threshold\n",
    "        x = x[xc[xi]]  # confidence\n",
    "        \n",
    "\n",
    "        # If none remain process next image\n",
    "        if not x.shape[0]:\n",
    "            continue\n",
    "\n",
    "        # Detections matrix nx6 (xyxy, conf, cls)\n",
    "        # cls is probs here\n",
    "        box, cls, mask = x.split((4, nc, nm), 1)\n",
    "\n",
    "\n",
    "        # ! Main modification here\n",
    "        conf, j = cls.max(1, keepdim=True)\n",
    "        x = torch.cat((box, conf, j.float(), mask), 1)[conf.view(-1) > conf_thres]\n",
    "\n",
    "        x_oh=torch.cat((box,cls),1)#[cls.view(-1)>conf_thres]\n",
    "\n",
    "        # Check shape\n",
    "        n = x.shape[0]  # number of boxes\n",
    "        if not n:  # no boxes\n",
    "            continue\n",
    "        \n",
    "        if n > max_nms:  # excess boxes\n",
    "            indices = x[:, 4].argsort(descending=True)[:max_nms]\n",
    "            x = x[indices]\n",
    "            x_oh = x_oh[indices]  #\n",
    "            #x = x[x[:, 4].argsort(descending=True)[:max_nms]]  # sort by confidence and remove excess boxes\n",
    "            \n",
    "\n",
    "        # Batched NMS\n",
    "        c = x[:, 5:6] * (0 if agnostic else max_wh)  # classes\n",
    "        scores = x[:, 4]  # scores\n",
    "        \n",
    "        boxes = x[:, :4] + c  # boxes (offset by class)\n",
    "        i = torchvision.ops.nms(boxes, scores, iou_thres)  # NMS\n",
    "        i = i[:max_det]  # limit detections\n",
    "\n",
    "        output[xi] = x[i]\n",
    "        output_oh[xi] = x_oh[i]\n",
    "\n",
    "    return output,output_oh"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "class YoloInference():\n",
    "    def __init__(self,weights,device=\"cuda\",verbose=False,load_model=True):\n",
    "        self.device=device\n",
    "        self.load_model=load_model\n",
    "        self.weights=weights\n",
    "        self.model=None\n",
    "        if load_model:\n",
    "            self.yolo=YOLO(weights,verbose=verbose).to(self.device)\n",
    "            self.model=self.yolo.model\n",
    "\n",
    "        self.model_type=None\n",
    "        if self.weights.find(\"yolov10\")!=-1:\n",
    "            self.model_type=\"yolov10\"\n",
    "        elif self.weights.find(\"yolo11\")!=-1:\n",
    "            self.model_type=\"yolo11\"\n",
    "\n",
    "\n",
    "        self.letter_box=LetterBox((384,384))\n",
    "        self.transform=T.Compose([T.ToTensor()])  \n",
    "\n",
    "        self.conf_mat=None\n",
    "\n",
    "        # load the conf matrix for eventual ensembling if it exists\n",
    "        conf_path=os.path.dirname(os.path.dirname(os.path.dirname(weights)))+\"/conf_matrix.npy\"\n",
    "        if os.path.exists(conf_path):\n",
    "           self.conf_mat=np.load(conf_path)\n",
    "    \n",
    "    def predict(self,img,verbose=False,conf=0.05,mc_nms=True,return_probs=False):\n",
    "        if self.model_type==\"yolov10\" or not return_probs:\n",
    "            return self.predict_base(img,verbose=verbose,conf=conf,mc_nms=mc_nms)\n",
    "        elif self.model_type==\"yolo11\":\n",
    "            return self.predict_yolo11_probs(img,verbose=verbose,conf=conf,mc_nms=mc_nms)\n",
    "        else:\n",
    "            print(\"Model type not recognized\")\n",
    "            return None\n",
    "\n",
    "    def predict_yolo11_probs(self,img,verbose=False,conf=0.05,mc_nms=True):\n",
    "        # img is a PIL or a path\n",
    "        if not self.load_model:\n",
    "            print(\"LOADING MODEL\")\n",
    "            self.yolo=YOLO(self.weights,verbose=verbose).to(self.device)\n",
    "            self.model=self.yolo.model\n",
    "\n",
    "        # Image loading\n",
    "        if isinstance(img,str):\n",
    "            img=Image.open(img)\n",
    "        img_np=self.letter_box(image=np.array(img))\n",
    "        transform=T.Compose([T.ToTensor()]) # \n",
    "        img_tensor=transform(img_np).unsqueeze(0).to(\"cuda\")\n",
    "\n",
    "        # inference\n",
    "        prediction,_ = self.model(img_tensor)  \n",
    "        \n",
    "        outputs,outputs_oh=non_max_suppression_modified(prediction, conf_thres=conf, iou_thres=0.4,agnostic=mc_nms)\n",
    "        outputs=outputs[0]\n",
    "        outputs_oh=outputs_oh[0]\n",
    "        \n",
    "\n",
    "        width,height=img.size\n",
    "        boxes,scores,labels,probs=[],[],[],[]\n",
    "        for out,out_oh in zip(outputs,outputs_oh):\n",
    "            out=out.cpu().numpy()\n",
    "            out_oh=out_oh.cpu().numpy()\n",
    "            box=out[:4]\n",
    "            x1,y1,x2,y2=box\n",
    "            # Rescale to fit the original image size\n",
    "            x1,y1,x2,y2=max(0,x1*width/384),max(0,y1*height/384),min(width,x2*width/384),min(height,y2*height/384)\n",
    "            score=out[4]\n",
    "            label=out[5]\n",
    "            prob=out_oh[4:]\n",
    "            boxes.append((np.round(x1).astype(int),np.round(y1).astype(int),np.round(x2).astype(int),np.round(y2).astype(int)))\n",
    "            scores.append(score)\n",
    "            labels.append(label)\n",
    "            probs.append(prob)\n",
    "        \n",
    "        boxes = np.asarray(boxes)\n",
    "        scores = np.asarray(scores)\n",
    "        labels = np.asarray(labels)\n",
    "        probs = np.asarray(probs)\n",
    "\n",
    "        \n",
    "        if not self.load_model:\n",
    "            self.unload_model()\n",
    "\n",
    "        return boxes,scores,labels,probs\n",
    "    \n",
    "\n",
    "    def predict_base(self,img,verbose=False,conf=0.05,mc_nms=True):\n",
    "        if not self.load_model:\n",
    "            print(\"LOADING MODEL\")\n",
    "            self.yolo=YOLO(self.weights,verbose=verbose).to(self.device)\n",
    "\n",
    "        result=self.yolo.predict(img,verbose=verbose,iou=0.4,conf=conf) # iou threshold for non-max suppression\n",
    "\n",
    "        # may deactivate mc_nms when using wbf in ensemble/tta\n",
    "        if mc_nms:\n",
    "            kept=nms(result[0].boxes.xyxy,result[0].boxes.conf,0.4) \n",
    "            boxes=result[0].boxes.xyxy[kept]\n",
    "            scores=result[0].boxes.conf[kept]\n",
    "            labels=result[0].boxes.cls[kept]\n",
    "        else:\n",
    "            boxes = result[0].boxes.xyxy\n",
    "            scores = result[0].boxes.conf\n",
    "            labels = result[0].boxes.cls\n",
    "        \n",
    "\n",
    "        boxes = boxes.cpu().numpy()\n",
    "        for i,box in enumerate(boxes):\n",
    "            x1,y1,x2,y2=box \n",
    "            boxes[i]=np.round(x1).astype(int),np.round(y1).astype(int),np.round(x2).astype(int),np.round(y2).astype(int)\n",
    "\n",
    "\n",
    "        scores=scores.cpu().numpy()\n",
    "        labels=labels.cpu().numpy()\n",
    "            \n",
    "        if not self.load_model:\n",
    "            self.unload_model()\n",
    "\n",
    "        return boxes,scores,labels\n",
    "\n",
    "    def unload_model(self):\n",
    "        \"\"\"Unload the model to free up memory.\"\"\"\n",
    "        self.yolo = None\n",
    "        self.model = None\n",
    "        torch.cuda.empty_cache() "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "To ensure compatibility with a wide range of computers, we have implemented an option to load models individually during inference, rather than loading all models at initialization. While this approach makes the model more accessible, it will significantly increase inference time, as model loading itself is a time-consuming task.\n",
    "\n",
    "Instructions on how to enable the \"one by one loader\" are provided in the section for filling the predictions CSV."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Detection models ensembling"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "To enhance the performance of our method, we decided to apply model ensembling to combine the predictions of multiple models. While ensembling is commonly used in classification tasks and is relatively straightforward, there is limited literature on its application in object detection.\n",
    "\n",
    "Inspired by the paper [Weighted boxes fusion: Ensembling boxes from different object detection models](https://arxiv.org/pdf/1910.13302), we developed a method to find the optimal bounding box by considering a list of boxes and their corresponding confidence scores. In the context of hematology, overlapping boxes are not permissible (as explained in the Non-Maximum Suppression section). Therefore, we included boxes with different predicted labels in our weighted box fusion algorithm and selected the best class based on the weighted score of each class.\n",
    "\n",
    "<img src=\"images/wbf.png\" alt=\"Weighted Box Fusion\" height=\"200\"/>\n",
    "\n",
    "Illustration of the weighted box fusion algorithm"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Weighted Box Fusion Algorithm"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "def compute_iou(box1, box2):\n",
    "        '''Function to compute the Intersection over Union (IoU) of two bounding boxes'''\n",
    "        x1_1, y1_1, x2_1, y2_1 = box1\n",
    "        x1_2, y1_2, x2_2, y2_2 = box2\n",
    "\n",
    "        inter_x1 = max(x1_1, x1_2)\n",
    "        inter_y1 = max(y1_1, y1_2)\n",
    "        inter_x2 = min(x2_1, x2_2)\n",
    "        inter_y2 = min(y2_1, y2_2)\n",
    "\n",
    "        inter_area = max(0, inter_x2 - inter_x1) * max(0, inter_y2 - inter_y1)\n",
    "\n",
    "        area1 = (x2_1 - x1_1) * (y2_1 - y1_1)\n",
    "        area2 = (x2_2 - x1_2) * (y2_2 - y1_2)\n",
    "\n",
    "        union_area = area1 + area2 - inter_area\n",
    "\n",
    "        return inter_area / union_area if union_area > 0 else 0\n",
    "\n",
    "def fuse_box(cluster):\n",
    "    boxes, sohs= cluster[\"boxes\"], cluster[\"soh\"]\n",
    "\n",
    "    label_score_sum = defaultdict(float) \n",
    "    for soh in sohs:\n",
    "        for label, score in enumerate(soh[:]):\n",
    "             label_score_sum[label] += score  \n",
    "        \n",
    "    best_label = max(label_score_sum, key=label_score_sum.get)\n",
    "\n",
    "    filtered = [(box, soh) for box, soh in zip(boxes, sohs) if np.argmax(soh) == best_label]\n",
    "\n",
    "    not_filtered = [(box, soh) for box, soh in zip(boxes, sohs)]    \n",
    "    x1_mean = sum(box[0] for box, _ in not_filtered) / len(not_filtered)\n",
    "    y1_mean = sum(box[1] for box, _ in not_filtered) / len(not_filtered)\n",
    "    x2_mean = sum(box[2] for box, _ in not_filtered) / len(not_filtered)\n",
    "    y2_mean = sum(box[3] for box, _ in not_filtered) / len(not_filtered)\n",
    "\n",
    "    soh_mean = sum(soh for _, soh in filtered) / len(boxes)\n",
    "\n",
    "    return {\"box\": [round(x1_mean), round(y1_mean), round(x2_mean), round(y2_mean)], \"soh\": soh_mean}\n",
    "        \n",
    "def wbf(boxes,sohs):\n",
    "    # Sort given boxes by their scores\n",
    "    max_scores = np.max(sohs,axis=1)\n",
    "    sorted_indices = np.argsort(max_scores)[::-1]\n",
    "\n",
    "    boxes=np.array(boxes)\n",
    "    sohs=np.array(sohs)\n",
    "\n",
    "    boxes = boxes[sorted_indices] # B in paper\n",
    "    sohs = sohs[sorted_indices] \n",
    "\n",
    "    # Elements of clusters are Dict of boxes and sohs\n",
    "    clusters=[] #L in paper\n",
    "    # Elements of fused_boxes are Dict of box and soh\n",
    "    fused_boxes=[] #F in paper\n",
    "\n",
    "    for box, soh in zip(boxes, sohs):\n",
    "        associated=False\n",
    "        for i,fused_box in enumerate(fused_boxes):\n",
    "            f_box=fused_box[\"box\"] \n",
    "            iou=compute_iou(box, f_box)\n",
    "            if iou>0.4: # was 0.55 in the paper\n",
    "                clusters[i][\"boxes\"].append(box)\n",
    "                clusters[i][\"soh\"].append(soh)\n",
    "                associated=True\n",
    "                # Computer new fused box\n",
    "                # Compute the new coordinates of the bounding box\n",
    "                fused_boxes[i]=fuse_box(clusters[i])\n",
    "                break\n",
    "\n",
    "        if not associated:\n",
    "            clusters.append({\"boxes\":[box],\"soh\":[soh]})\n",
    "            fused_boxes.append({\"box\":box,\"soh\":soh})\n",
    "\n",
    "    # Return the boxes, scores and labels, np array or not ?\n",
    "    boxes = np.array([fused_box[\"box\"] for fused_box in fused_boxes])\n",
    "    scores = np.array([np.max(fused_box[\"soh\"]) for fused_box in fused_boxes])\n",
    "    labels = np.array([np.argmax(fused_box[\"soh\"]) for fused_box in fused_boxes])\n",
    "\n",
    "    return boxes, scores, labels\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Yolo Ensembling"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "class EnsembleYolo:\n",
    "    def __init__(self, models:list[YoloInference],use_probs=False):\n",
    "        self.models = models\n",
    "        self.use_probs=use_probs\n",
    "\n",
    "        self.meta_model=None\n",
    "            \n",
    "\n",
    "    def predict(self,img,conf=0.00001,verbose=False):\n",
    "        \"\"\"Ensemble inference to get prediction as an integer (batch)\"\"\"\n",
    "        all_boxes=[]\n",
    "        all_soh=[]\n",
    "\n",
    "        for model in self.models:\n",
    "            if model.model_type==\"yolov10\" or not self.use_probs:\n",
    "                boxes,scores,preds=model.predict(img,verbose=verbose,conf=conf,mc_nms=False) \n",
    "            else:\n",
    "                boxes,scores,preds,probs=model.predict(img,verbose=verbose,conf=conf,mc_nms=True,return_probs=True)\n",
    "            all_boxes.append(boxes)\n",
    "\n",
    "            sohs=[]\n",
    "\n",
    "            if not self.use_probs or model.model_type==\"yolov10\":\n",
    "                for score,pred in zip(scores,preds):\n",
    "                    soh=F.one_hot(torch.tensor(pred).long(), num_classes=23)*score\n",
    "                    sohs.append(soh)\n",
    "            else:\n",
    "                for prob in probs:\n",
    "                    #convert prob to tensor\n",
    "                    prob=torch.tensor(prob)\n",
    "                    sohs.append(prob)\n",
    "\n",
    "            all_soh.append(sohs)\n",
    "\n",
    "        list_all_boxes = [box for boxes in all_boxes for box in boxes]\n",
    "        list_all_soh = [soh for sohs in all_soh for soh in sohs]\n",
    "\n",
    "        return wbf(list_all_boxes,list_all_soh)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Filtering of boxes "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In the test.csv, we know the exact number of detections required for each image. Our strategy was to set a very low threshold for the YOLO model to ensure that we predict more boxes than the actual number of boxes in the image. We then retained only the boxes with the highest scores."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "def filter_boxes(occurences,boxes,scores,labels):\n",
    "    data = list(zip(scores, boxes, labels))\n",
    "    data.sort(reverse=True, key=lambda x: x[0])\n",
    "    data = data[:len(occurences)]\n",
    "    scores, boxes, labels = zip(*data)\n",
    "    return list(boxes), list(scores), list(labels)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Inference pipeline to fill the csv"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This is the main pipeline for processing all the test images and filling the CSV.\n",
    "\n",
    "You can download the best pretrained weights with this link : [Google Drive](https://drive.google.com/drive/folders/1gDwqRtLoKqwLIaGFd2SwPffzibOtIEmx?usp=sharing)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  0%|          | 0/20751 [00:00<?, ?it/s]/tmp/ipykernel_5483/3971531490.py:64: FutureWarning: Setting an item of incompatible dtype is deprecated and will raise an error in a future version of pandas. Value 'LLC' has dtype incompatible with float64, please explicitly cast to a compatible dtype first.\n",
      "  df.loc[df[\"trustii_id\"] == trustii_id, [\"x1\", \"y1\", \"x2\", \"y2\", \"class\"]] = [x1, y1, x2, y2, cls]\n",
      "  0%|          | 29/20751 [00:03<25:38, 13.47it/s]  "
     ]
    }
   ],
   "source": [
    "# Mapping of the classes\n",
    "itos={0:'B', 1:'BA', 2:'EO', 3:'Er', 4:'LAM3', 5:'LF', 6:'LGL', 7:'LH_lyAct', 8:'LLC', 9:'LM', 10:'LY', 11:'LZMG', 12:'LyB', 13:'Lysee', 14:'M', 15:'MBL', 16:'MM', 17:'MO', 18:'MoB', 19:'PM', 20:'PNN', 21:'SS', 22:'Thromb'}\n",
    "\n",
    "# PATH TO YOUR IMAGES\n",
    "data_path=\"data/Cytologia/images\"\n",
    "\n",
    "# PATH TO THE TEST CSV\n",
    "test_csv_path=\"test.csv\"\n",
    "\n",
    "# PATH WHERE THE PREDICTIONS WILL BE SAVED\n",
    "csv_path=\"predictions.csv\"\n",
    "\n",
    "if test_csv_path != csv_path:\n",
    "    shutil.copy(test_csv_path,csv_path)\n",
    "\n",
    "# Load the csv\n",
    "df = pd.read_csv(csv_path)\n",
    "\n",
    "# Create the YOLO models and load them if possible, else set the parameter load_model to False\n",
    "yolo_engine1=YoloInference(\"models/best/yolo11n384_blk.pt\",device=\"cuda\",load_model=True) # load_model=False => add this parameter to avoid loading the model if your computer has limited memory\n",
    "yolo_engine2=YoloInference(\"models/best/yolo11m384_iou.pt\",device=\"cuda\",load_model=True) # load_model=False => add this parameter to avoid loading the model if your computer has limited memory\n",
    "yolo_engine3=YoloInference(\"models/best/yolov10n384_blk.pt\",device=\"cuda\",load_model=True) # load_model=False => add this parameter to avoid loading the model if your computer has limited memory\n",
    "yolo_engine4=YoloInference(\"models/best/yolov10m384_iou.pt\",device=\"cuda\",load_model=True) # load_model=False => add this parameter to avoid loading the model if your computer has limited memory\n",
    "yolo_engine5=YoloInference(\"models/best/yolo11x384_iou.pt\",device=\"cuda\",load_model=True) # load_model=False => add this parameter to avoid loading the model if your computer has limited memory\n",
    "yolo_engine6=YoloInference(\"models/best/yolov10s384_iou.pt\",device=\"cuda\",load_model=True) # load_model=False => add this parameter to avoid loading the model if your computer has limited memory\n",
    "yolo_engine7=YoloInference(\"models/best/yolo11n384_nc.pt\",device=\"cuda\",load_model=True) # load_model=False => add this parameter to avoid loading the model if your computer has limited memory\n",
    "\n",
    "ens_engine=EnsembleYolo([yolo_engine1,yolo_engine2,yolo_engine3,yolo_engine4,yolo_engine5,yolo_engine6,yolo_engine7],use_probs=False)\n",
    "\n",
    "# Add the columns if they don't exist\n",
    "if not {'x1', 'y1', 'x2', 'y2', 'class'}.issubset(df.columns):\n",
    "    for col in ['x1', 'y1', 'x2', 'y2', 'class']:\n",
    "        df[col] = np.nan\n",
    "\n",
    "# Get unique NAME (aka the image names)\n",
    "names = df[\"NAME\"].unique()\n",
    "tqdm_names= tqdm(names)\n",
    "\n",
    "# Loop over all the images\n",
    "for name in tqdm_names:\n",
    "    img_path=f\"{data_path}/{name}\"\n",
    "\n",
    "    # Count the number of occurences of the image in the dataframe to get the number of cells to predict\n",
    "    occurences = df[df[\"NAME\"]==name]\n",
    "    # Get the corresponding trustii_ids\n",
    "    trustii_ids = occurences[\"trustii_id\"].tolist()\n",
    "\n",
    "    # Inference\n",
    "    boxes,scores,labels=ens_engine.predict(img_path,conf=0.00001,verbose=False) \n",
    "\n",
    "    boxes=list(boxes)\n",
    "    scores=list(scores)\n",
    "    labels=list(labels)\n",
    "    \n",
    "    # Filter the boxes if there are more boxes than cells\n",
    "    if len(occurences) < len(boxes):\n",
    "        boxes,scores,labels=filter_boxes(occurences,boxes,scores,labels)\n",
    "\n",
    "    # Update the dataframe with the predictions\n",
    "    for idx,(box,label) in enumerate(zip(boxes,labels)):\n",
    "        x1,y1,x2,y2 = box\n",
    "        trustii_id = trustii_ids[idx]\n",
    "        cls=itos[label]\n",
    "        df.loc[df[\"trustii_id\"] == trustii_id, [\"x1\", \"y1\", \"x2\", \"y2\", \"class\"]] = [x1, y1, x2, y2, cls]\n",
    "\n",
    "    # If there are more cells in the image than predicted (not supposed to happen since we set the confidence threshold to 0.00001, but still necessary just in case)\n",
    "    for i in range(len(boxes),len(occurences)):\n",
    "        trustii_id = trustii_ids[i]\n",
    "        df.loc[df[\"trustii_id\"] == trustii_id, [\"x1\", \"y1\", \"x2\", \"y2\", \"class\"]] = [0, 0, 0, 0, 'PNN']\n",
    "\n",
    "df.to_csv(csv_path, index=False)\n",
    "print(\"CSV prediction file saved in \",csv_path)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Training method explained"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "A significant part of our results is due to the training method we employed. As mentioned earlier, we chose YOLO as our backbone model to ensure low inference time while maintaining decent performance.\n",
    "\n",
    "After visualizing parts of the dataset, we concluded that cleaning the data would improve the convergence and stability of the training process. This decision was driven by several factors:\n",
    "\n",
    "- White blood cells (WBCs) located at the edges of images are sometimes annotated and sometimes not, which can lead to confusion during training.\n",
    "- A portion of the dataset contains annotation errors, including misplaced bounding boxes, bounding boxes outside the image, or missing annotations.\n",
    "\n",
    "The quality of the classification for each cell was not evaluated, as our team lacks the biological expertise to accurately classify cells with certainty. However, such noise in the dataset, if present, could significantly affect performance."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Dataset cleaning"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "To clean the training dataset, we trained a YOLO model using cross-validation and cleaned the validation data from each fold. This approach ensures that our model does not overfit the training data and can be effectively used for data curation.\n",
    "\n",
    "The annotations were formatted in the standard YOLO format, and the 5 models (k=5 for our cross-validation) were trained for 250 epochs using the [Ultralytics Library](https://docs.ultralytics.com/models/yolo11/).\n",
    "\n",
    "\n",
    "After training, we cleaned the dataset as follows:\n",
    "\n",
    "- For each YOLO model, we predicted bounding boxes and classes on the validation data.\n",
    "\n",
    "- We matched the ground truth boxes provided in the train.csv with the YOLO predictions, retaining the YOLO bounding boxes (which were empirically more accurate than the manual annotations) while preserving the ground truth class.\n",
    "- For YOLO detections with a sufficient score that did not match any ground truth boxes, we masked the corresponding part of the image with a black mask to ensure no unannotated cells remained in the training dataset (whether they were WBCs at the border or unannotated WBCs) (see left figure).\n",
    "- For ground truth boxes with no matching YOLO boxes (IoU < 0.4), we removed these bounding boxes from the dataset, assuming they were incorrect annotations (see right figure).\n",
    "\n",
    "The figure below illustrates the cleaning procedure we described.\n",
    "\n",
    "<img src=\"images/curation1.png\" alt=\"Curation process 1\" height=\"400\"/>    <img src=\"images/curation2.png\" alt=\"Curation process 2\" height=\"400\"/>  \n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**What Didn’t Work ?** : We also experimented with a more aggressive curation process, aiming to mask cells where the ground truth label and the YOLO-predicted label differed significantly. We implemented two approaches for this: soft-labels and hard-labels, intending to remove potential misannotations made by the labelers.\n",
    "\n",
    "- Hard labels: This approach simply checks if the predicted label matches the ground truth. If not, the corresponding box is masked.\n",
    "- Soft labels: This method is more lenient, verifying whether the predicted and ground truth labels belong to the same family of cells. The cell families were defined based on input from the AI chatbot of the challenge and other online resources.   \n",
    "\n",
    "However, both methods led to a decrease in performance. Some cell types were frequently misclassified, resulting in a highly unbalanced dataset that negatively impacted the training process."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Training hyperparameters, augmentations etc ..."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "To train the model effectively, we tested different settings for the training process. This section will outline our assumptions and the tests we conducted to validate them. Finally, we will describe the final training setup that we used."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Augmentations**: First, it’s important to note that our baseline for augmentations was the set of default augmentations provided by the Ultralytics library, which are applied when no specific augmentation parameters are set:\n",
    "\n",
    "- *Mosaic*: This augmentation creates new training images by combining patches from different images into a single mosaic. Mosaic augmentation is disabled during the last 10 epochs of training.\n",
    "\n",
    "- *HSV* (hue, saturation, and value): These three augmentation techniques affect the hue, saturation, and value of an image. Given that the classification of white blood cells heavily depends on the staining used to color the cells, we initially attempted to disable these transformations. After testing, we kept the default values for saturation and value but set the hue to 0, even though the default hue value was already very small.\n",
    "\n",
    "- *Geometrical augmentations* (translate, scale, flip): We kept all other default augmentations, as they are primarily geometric and unlikely to hinder the training process in our case.\n",
    "\n",
    "- *Deactivated transformations*: The transformations that were disabled by default were not modified.\n",
    "\n",
    "- *MixUp*: Since some classes were very similar to each other, we extensively experimented with the MixUp augmentation to soften the class boundaries. Specifically, we tested different MixUp thresholds and tried stopping it before the end of training. Our experiments showed that MixUp did not improve performance for our specific problem."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Selection of models** : To enhance model performance, we employed model ensembling as described in the inference section. However, for optimal ensembling, it is crucial to choose models that complement each other. We experimented with various configurations and ultimately chose a mix of different YOLOv11 sizes combined with various YOLOv10 sizes. Since YOLOv11 and YOLOv10 produce similar results but use different methods, our experiments demonstrated that they complement each other well.\n",
    "\n",
    "We specifically chose relatively small versions of YOLO11 and YOLOv10 to strike a balance between inference speed and accuracy: YOLO11n (size 384), YOLO11m (size 384), and YOLOv10n (size 384). The only exception is YOLOv11x, which is a larger model compared to the others. However, being a YOLO model, its inference speed still remains relatively fast.\n",
    "\n",
    "All models were fine-tuned using an automated dataset curation process. Details of this process are provided in the training section of the notebook, where we outline our strategy for optimizing YOLO models to achieve the best possible performance.\n",
    "\n",
    "We used the following models for our best solution :\n",
    "\n",
    "\n",
    "**Models trained with iou curation**: The models `yolo11m384_iou.pt`,`yolo11x384_iou.pt`,`yolov10s384_iou.pt` and `yolov10m384_iou.pt` were trained on a dataset where ground truth boxes with a low IoU to the predicted ones were masked.  \n",
    "**Models trained with simple curation**: The models `yolo11m384_blk.pt` and `yolov10n384_blk.pt` were trained on a dataset where ground truth boxes were kept, even if they had a low IoU with the predicted ones.  \n",
    "**Model not curated**: The `yolo11n384_nc.pt` model was trained on the raw data from the CSV, with only minor corrections made for bounding boxes that were out of the image.\n",
    "\n",
    "You can download the best pretrained weights with this link : [Google Drive](https://drive.google.com/drive/folders/1gDwqRtLoKqwLIaGFd2SwPffzibOtIEmx?usp=sharing)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Hyperparameters and training** : We trained all our models for 250 epochs with a batch size of 64 (or 32 for larger models, such as YOLOv11l). We reduced the image size from 640 to 384 and used a validation split of 5% of the data to maximize the amount of data available for training."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "hemat",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
